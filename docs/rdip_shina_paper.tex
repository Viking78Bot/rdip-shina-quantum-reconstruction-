\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{geometry}
\geometry{margin=1in}
\DeclareMathOperator{\Tr}{Tr}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\title{%
RD-IP and Shina:\\
Adaptive Neural Reconstruction of Quantum States\\
via Incremental Projections and Learned Trust-Region Control}

\author[1]{Dmitrii Shultsev\thanks{Email: \texttt{d.shultsev@gmail.com}}}
\affil[1]{Independent Researcher}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We introduce RD-IP (Recurrent Diffuser -- Incremental Projection), a hybrid classical--neural framework for reconstructing many-body quantum states from local, noisy measurements without explicitly storing the full $2^N$-dimensional wave function.
The core idea is a new representation, DIPR (Dima's Incremental Projection Ruler), in which a one-dimensional system is encoded not by a global state $\rho$ but by a sequence of overlapping reduced density matrices (RDMs) $\{\rho_{i,i+1}\}$ that must satisfy consistency constraints on their shared single-site marginals.
A lightweight denoising network (DiffuserHead) cleans local RDMs in the Pauli basis, while a recurrent ``carriage'' network (RulerNet) traverses the chain and enforces epure-like continuity between neighbouring blocks.
On top of this, we introduce \emph{Shina}, a learned, line-search-based trust-region controller that adaptively scales per-bond update steps to prevent catastrophic energy overshoot, a well-known pathology of non-variational reconstruction methods.
On synthetic two-qubit RDM banks, Ising chains, and critical Heisenberg and TFIM benchmarks up to $N=50$ sites, RD-IP with Shina significantly reduces energy error and marginal inconsistency compared to naive denoising, while preserving or improving local correlator fidelity.
We argue that this epure-based, incremental-projection view provides a scalable alternative to full tomography in the NISQ regime and a natural starting point for graph-based generalizations towards realistic quantum chemistry problems such as Fe--S clusters and FeMoco.
\end{abstract}

\section{Introduction}

Characterizing many-body quantum states prepared on noisy intermediate-scale quantum (NISQ) devices is notoriously difficult.
Full quantum state tomography scales as $O(4^N)$ in both measurements and classical memory, becoming essentially impossible beyond $N \sim 14$--$20$ qubits.
Compressed sensing and classical shadows alleviate this scaling for certain observables, but they do not in general provide a generative model of the state that respects positivity, trace constraints, and global consistency of reduced density matrices (RDMs).

At the same time, most physically relevant Hamiltonians are local and two-body:
energies and local observables are functionals of the one- and two-particle RDMs, $D^{(1)}$ and $D^{(2)}$, rather than of the full $2^N$-dimensional density matrix.
This naturally leads to \emph{marginal-based} approaches:
instead of reconstructing $\rho$ directly, one tries to infer or denoise a set of low-rank marginals.
However, the \emph{quantum marginal problem} and the associated $N$-representability constraints make this nontrivial: not every collection of local RDMs arises from a global physical state.

In this work we propose RD-IP, a reconstruction framework that embraces this marginal viewpoint and recasts it in a geometric language inspired by structural mechanics.
Instead of thinking of a one-dimensional chain as a single high-dimensional object, we treat it as a sequence of overlapping ``sections'', each described by a two-site RDM.
The central idea is that these local sections must match on overlaps, much like bending-moment diagrams (epures) must be continuous at beam junctions.
We call this representation DIPR: Dima's Incremental Projection Ruler.

RD-IP combines three components:
(i) a local denoiser (DiffuserHead) that maps noisy two-qubit Pauli vectors to cleaned RDMs;
(ii) a recurrent carriage (RulerNet) that sweeps along the chain and enforces consistency of overlapping marginals;
and (iii) Shina, a learned trust-region controller that adaptively chooses per-bond step sizes via a small neural line search.
Together, they form an epure-based, incremental-projection scheme that can be trained end-to-end on banks of synthetic and physical RDMs.

\subsection*{Contributions}

Conceptually and technically, this paper makes the following contributions:

\begin{itemize}
  \item We formalize DIPR, an incremental-projection representation of one-dimensional quantum states as an ordered sequence of overlapping $2$-RDMs $\{\rho_{i,i+1}\}$, and define an ``epure overlap'' metric that quantifies marginal inconsistency between neighbouring blocks.
  \item We introduce RD-IP, a hybrid neural architecture in which a lightweight DiffuserHead denoises local Pauli vectors, while a recurrent RulerNet traverses the chain, carrying a hidden state that encodes accumulated constraints and enforcing left/right marginal agreement.
  \item We propose Shina, an adaptive step-size regulator that uses a neural line-search teacher to learn per-bond trust-region steps, preventing energy overshoot and stabilizing reconstruction without hand-tuned global learning rates.
  \item We demonstrate, on synthetic two-qubit RDMs and on Ising, Heisenberg, and TFIM chains up to $N=50$ sites, that RD-IP + Shina substantially improves energy error, epure overlap, and local correlator RMSE compared to noisy baselines and naive denoising, while avoiding catastrophic variational violations.
  \item We discuss limitations (notably on strongly chaotic and critical TFIM instances) and outline how the epure + Shina mechanism can be generalized to graph-structured systems and realistic Fe--S clusters, forming the basis of a second, chemistry-focused paper.
\end{itemize}

\section{Background: Marginals, N-Representability, and Epures}

\subsection{Quantum marginals and the N-representability problem}

Consider an $N$-qubit system with global density matrix $\rho$ on $\mathcal{H} = (\mathbb{C}^2)^{\otimes N}$.
For a local region $A$, its reduced density matrix is $\rho_A = \Tr_{\bar{A}} \rho$.
A natural reconstruction strategy is to estimate a set of local marginals $\{\rho_{A_k}\}$ from measurements and then infer $\rho$ consistent with all of them.

The \emph{N-representability problem} asks: given a collection of reduced density matrices $\{\rho_{A_k}\}$, does there exist a global $\rho \ge 0$ with $\Tr \rho = 1$ such that all $\rho_{A_k}$ arise as partial traces?
Even for two-particle RDMs of fermions, deciding $N$-representability is QMA-complete.
In practice, one typically enforces only necessary conditions (positivity of various contractions, $P$, $Q$, $G$ conditions, etc.) and hopes that the resulting state is close to physical.

In local tomography and in many neural approaches, marginals are often estimated independently and later combined.
This leads to \emph{marginal inconsistency}:
for overlapping regions $A$ and $B$, the induced RDMs on $A\cap B$ may disagree.
In a chain, this shows up as a mismatch between $\Tr_{i-1} \rho_{i-1,i}$ and $\Tr_{i+1} \rho_{i,i+1}$; we will treat such mismatches as ``epure discontinuities''.

\subsection{Incremental projections and the epure picture}

DIPR reframes reconstruction as an incremental projection problem:
instead of solving directly for $\rho$ in one shot, we move along the system, projecting local blocks back toward a consistent manifold.

For a one-dimensional chain of $N$ sites, DIPR uses the ordered set
\begin{equation}
  \mathcal{D} = \left\{ \rho_{1,2}, \rho_{2,3}, \dots, \rho_{N-1,N} \right\},
\end{equation}
with overlap constraints
\begin{equation}
  \Tr_{i-1}(\rho_{i-1,i}) \approx \Tr_{i+1}(\rho_{i,i+1}) \quad \forall\, i = 2,\dots,N-1.
\end{equation}
We define an \emph{epure overlap} loss
\begin{equation}
  \mathcal{L}_{\text{epure}} = \sum_{i=2}^{N-1} \norm{ \Tr_{i-1}(\rho_{i-1,i}) - \Tr_{i+1}(\rho_{i,i+1}) }_1,
\end{equation}
analogous to the continuity requirement for bending-moment diagrams (epures) in structural mechanics:
internal forces must match at junctions.
Here the ``internal state'' is the single-site RDM of the shared site $i$.

The incremental projection viewpoint is:
\begin{enumerate}
  \item Start from a noisy or approximate set of blocks $\{\tilde{\rho}_{i,i+1}\}$ estimated from data.
  \item For each $i$ in sequence, locally project $\tilde{\rho}_{i,i+1}$ towards the set of physical two-qubit states \emph{and} towards consistency with the previously corrected block.
  \item Carry forward a low-dimensional summary (hidden state) of the constraints, updating it as we move.
\end{enumerate}
RD-IP implements this as a hybrid of neural denoising (local projection) and recurrent message passing (constraint propagation).

\section{RD-IP Architecture}

\subsection{Local representation: Pauli vectors and two-qubit RDMs}

We represent each two-qubit RDM $\rho_{i,i+1}$ in the Pauli basis.
Any $2$-qubit state can be written as
\begin{equation}
  \rho = \frac{1}{4} \sum_{\mu,\nu \in \{I,X,Y,Z\}} c_{\mu\nu} \, \sigma_\mu \otimes \sigma_\nu,
\end{equation}
where $c_{\mu\nu} \in \mathbb{R}$ are expectation values of Pauli operators.
Collecting the $c_{\mu\nu}$ into a vector $v \in \mathbb{R}^{16}$ (or the traceless subspace of dimension $15$) yields a compact, measurement-aligned representation:
simulation and experiments naturally provide estimates of these expectation values.

In training, we generate clean RDMs from either synthetic random states or exact ground states of local Hamiltonians (Ising, Heisenberg, Level1 TFIM, etc.), convert them to Pauli vectors $v_{\text{clean}}$, and corrupt them with a parametric noise model (depolarization, amplitude damping, additive Gaussian noise) to obtain $v_{\text{noisy}}$.

\subsection{DiffuserHead: local denoising map}

The DiffuserHead is a small multi-layer perceptron (MLP) $f_\phi$ that maps noisy Pauli vectors to denoised ones:
\begin{equation}
  \hat{v}_{\text{clean}} = f_\phi(v_{\text{noisy}}).
\end{equation}
It is trained on a large hybrid dataset that mixes:
\begin{itemize}
  \item synthetic two-qubit RDMs (random pure and mixed states),
  \item physically realized blocks extracted from exact or DMRG ground states of Ising and Heisenberg chains (``Ising/Heis banks''),
  \item blocks from more complex Level1 tasks (e.g.\ TFIM at criticality),
  \item and, in later stages, molecular Fe--S active-space RDMs.
\end{itemize}

The loss is primarily reconstruction error in Pauli space (MSE + $L_1$), but we also encourage identity passes on near-pure inputs by sampling a fraction $p_{\text{identity}}$ of examples with zero noise.
This teaches the DiffuserHead not to ``over-correct'' already clean data.

\subsection{RulerNet: recurrent carriage along the chain}

Given denoised local vectors $\hat{v}_{i,i+1}$, RulerNet enforces consistency along the chain.
At each bond $i$, it takes as input the local vector and a hidden state summarizing previous bonds:
\begin{align}
  h_i &= \mathrm{GRU}(\hat{v}_{i,i+1}, h_{i-1}), \\
  \Delta v_i &= W_{\Delta} h_i + b_{\Delta}, \\
  v^{\text{raw}}_i &= \hat{v}_{i,i+1} + \Delta v_i.
\end{align}
The corrected Pauli vector $v^{\text{raw}}_i$ is then converted back to an RDM $\rho^{\text{raw}}_{i,i+1}$, and local observables and overlaps are computed.

RulerNet is trained jointly with the DiffuserHead (or on top of a frozen head) to minimize a composite loss:
\begin{equation}
  \mathcal{L}_{\text{Ruler}} = \lambda_{\text{MSE}} \, \mathcal{L}_{\text{Pauli}} +
  \lambda_{\text{epure}} \, \mathcal{L}_{\text{epure}} +
  \lambda_{\text{E}} \, \mathcal{L}_{\text{energy}} +
  \lambda_{\text{O}} \, \mathcal{L}_{\text{overlap}},
\end{equation}
where $\mathcal{L}_{\text{Pauli}}$ measures Pauli-space reconstruction error, $\mathcal{L}_{\text{epure}}$ the epure overlap, $\mathcal{L}_{\text{energy}}$ the deviation of reconstructed energy from exact, and $\mathcal{L}_{\text{overlap}}$ penalizes large marginal inconsistencies.

This recurrent ``carriage'' perspective matches the DIPR idea:
we move a finite-capacity window (the GRU hidden state) along the chain, incrementally refining the local blocks while keeping track of accumulated constraints.

\subsection{Shina: learned trust-region and line-search controller}

Naively applying the full correction $\Delta v_i$ at each bond can cause severe instabilities, especially on critical or chaotic systems.
In earlier experiments, this manifested as \emph{energy overshoot}: reconstructed energies fell well below the exact ground-state energy, indicating a gross violation of N-representability.

Shina is introduced to address this.
Instead of directly using $v^{\text{raw}}_i$, we form a convex combination between the noisy input and the corrected proposal:
\begin{equation}
  v^{\text{out}}_i(\alpha_i) = v_{\text{noisy},i} + \alpha_i \left( v^{\text{raw}}_i - v_{\text{noisy},i} \right),
\end{equation}
where $\alpha_i \in [0,1]$ is a per-bond step size.

\paragraph{Teacher line search.}
During training, for each bond we perform a small discrete line search over a set of candidate steps $\{\alpha^{(k)}\}$ (e.g.\ $0, 0.05, 0.1, 0.25, 0.5, 0.75, 1$).
For each candidate we compute a local energy- and overlap-aware loss
\begin{equation}
  \mathcal{L}^{(k)}_i = \lambda_{\text{MSE}} \, \norm{v^{\text{out}}_i(\alpha^{(k)}) - v^{\text{true}}_i}^2
  + \lambda_{\text{E}} \, (E_i(\alpha^{(k)}) - E^{\text{true}})^2
  + \lambda_{\text{epure}} \, \mathcal{L}_{\text{epure},i}(\alpha^{(k)}),
\end{equation}
and choose the best $\alpha^\star_i$ that minimizes $\mathcal{L}^{(k)}_i$.
This $\alpha^\star_i$ plays the role of a teacher signal.

\paragraph{ShinaNet.}
We then train a small network ShinaNet to predict $\alpha_i$ from local features:
\begin{equation}
  \alpha_i = \mathrm{ShinaNet}\big( v_{\text{noisy},i}, v^{\text{raw}}_i, v^{\text{raw}}_i - v_{\text{noisy},i}, \text{meta}_i \big),
\end{equation}
where $\text{meta}_i$ encodes Hamiltonian parameters (e.g.\ $J$, $\Delta$, $h$) and flags the dataset type (Ising, Heis, Level1).
Shina is optimized using an $L_2$ loss against the teacher $\alpha^\star_i$, plus regularizers that encourage small steps when local changes worsen energy or overlap.

In practice, a small number of Shina iterations ($K=1$ or $2$) per bond suffices:
the first iteration acts as a predictor, the second as a corrector, akin to a learned trust-region method.

\section{Experiments}

\subsection{Datasets and noise model}

We evaluate RD-IP and Shina on three tiers:

\begin{itemize}
  \item \textbf{Level 0:} synthetic two-qubit RDMs generated from random pure and mixed states, serving as a basic denoising sanity check.
  \item \textbf{Level 1:} ground states of one-dimensional local Hamiltonians: transverse-field Ising (TFIM) chains with $N$ up to 50 and Heisenberg (XXX/XXZ) chains with $N$ up to 32, obtained via exact diagonalization or DMRG.
  \item \textbf{Level 1.5 (pre-chemistry):} more challenging spin-chain Hamiltonians with stronger fields and anisotropies, used mainly for curriculum fine-tuning.
\end{itemize}

From each ground state we extract all nearest-neighbour two-site RDMs $\rho_{i,i+1}$, convert them to Pauli vectors, and apply a parameterized noise model:
\begin{itemize}
  \item random two-qubit depolarization with strength $\in [0, p_{\max}]$,
  \item amplitude damping with rate $\in [0, \gamma_{\max}]$,
  \item additive Gaussian noise in Pauli space with standard deviation $\sigma$,
  \item and an identity branch with probability $p_{\text{identity}}$ (no noise).
\end{itemize}

\subsection{Metrics}

We report the following metrics:

\begin{itemize}
  \item Energy error $\Delta E = E_{\text{rec}} - E_{\text{exact}}$, where $E_{\text{rec}}$ is computed from reconstructed local RDMs and the known Hamiltonian.
  \item RMSE of local correlators, e.g.\ nearest-neighbour $\langle \sigma^z_i \sigma^z_{i+1} \rangle$ and $\langle \sigma^x_i \rangle$, compared to exact values.
  \item Average two-site fidelity between reconstructed and true RDMs, and its distribution.
  \item Epure overlap: mean single-site marginal discrepancy between neighbouring blocks.
  \item Step statistics: distribution of Shina's predicted $\alpha_i$ over bonds and instances.
\end{itemize}

We pay particular attention to \emph{energy overshoot}: large negative $\Delta E$ relative to the exact ground-state energy, indicating non-variational, unphysical reconstructions.

\subsection{Results on Ising and Heisenberg chains}

On small Ising chains (e.g.\ $N=12$ at $h=0.5$), RD-IP with Shina behaves as a strong, stable denoiser.
Relative to noisy inputs, it reduces energy error and correlator RMSE and increases mean two-site fidelity, with Shina predicting moderate steps ($\alpha \approx 0.4$) and narrow variance.

On the $N=32$ Heisenberg chain (Heis32), earlier versions of the pipeline (without Level1-aware fine-tuning and with naive step control) exhibited catastrophic overshoot:
for an aggressive global step $\alpha=1$, energy errors as large as $\Delta E \approx -3.7$~Ha were observed, and even at $\alpha=0.25$ overshoot remained significant.
After retraining the DiffuserHead on a multi-bank mixture (random + Ising + Heis + Level1) and jointly optimizing RulerNet with Shina's line-search teacher, the behaviour changes qualitatively:
\begin{itemize}
  \item For moderate effective steps (Shina-predicted $\alpha \approx 0.3$â€“$0.4$), energy errors shrink to $|\Delta E| \lesssim 0.4$~Ha, corresponding to $\sim 4\times10^{-3}$~Ha per site.
  \item RMSE of nearest-neighbour $\langle \sigma^z \sigma^z \rangle$ correlators is reduced compared to noisy inputs, while $\langle \sigma^x \rangle$ errors remain comparable or slightly improved.
  \item Epure overlap decreases markedly: marginal inconsistencies drop from $\mathcal{O}(10^{-1})$ to $\mathcal{O}(10^{-2})$, indicating that the reconstructed DIPR epure is nearly continuous.
\end{itemize}

For more extreme global $\alpha$, Shina acts as a damper:
even when the external hyperparameter suggests a full step, internal $\alpha_i$ predictions cluster around $\sim 0.4$, preventing the system from falling back into the overshoot regime seen in earlier baselines.

\subsection{Results on TFIM Level1 ($N=50$)}

The $N=50$ TFIM in regimes near criticality and with strong fields represents a heavy-tailed, out-of-distribution test for RD-IP.
Here the model must contend with long-range, algebraically decaying correlations and a complicated energy landscape.

On such instances, noisy inputs can already exhibit large negative $\Delta E$, reflecting the difficulty of the task.
Naive RD-IP without Shina often exacerbates this, driving energies further down and damaging local structure.
With Level1-aware training and Shina's learned trust-region, we observe:
\begin{itemize}
  \item A substantial reduction in overshoot magnitude: from $\sim -30$~Ha in naive variants to roughly $-18$~Ha in the fine-tuned RD-IP + Shina model.
  \item Improved RMSE for $\langle \sigma^z \sigma^z \rangle$ correlators compared to noisy baselines, at the cost of slightly worse $\langle \sigma^x \rangle$.
  \item Epure overlap improves, though not to the near-ideal levels seen on integrable or mildly perturbed chains.
\end{itemize}
These results indicate that while the Level1-aware RD-IP has not fully solved the hardest chaotic instances, it moves firmly into a regime where it improves upon noisy measurements rather than harming them.

\section{Discussion and Limitations}

RD-IP with Shina represents a significant step towards scalable, physically constrained neural reconstruction of many-body quantum states, but several limitations remain.

First, the current architecture is explicitly one-dimensional:
RulerNet's recurrence and the epure definition assume a linear ordering.
For spin chains and quasi-1D systems this is appropriate, but realistic molecules and lattice models require graph-structured generalizations.

Second, we enforce N-representability only implicitly through training data and soft losses.
We do not yet project reconstructed RDMs onto the convex set of valid marginals via semidefinite programming or differentiable purification.
The observed elimination of large negative overshoot on Heis32 suggests that the implicit constraints and Shina's trust-region behaviour are powerful, but for chemically accurate energies (mHa scale) explicit enforcement will likely be necessary.

Third, performance on the hardest Level1 TFIM instances remains limited:
energy overshoot is reduced but not eliminated, and the balance between different correlators is imperfect.
This suggests a need for:
\begin{itemize}
  \item richer feature sets (e.g.\ including multi-site coarse-grained observables),
  \item scale-invariant curricula that expose the model to larger system sizes during training,
  \item and perhaps architectural changes (e.g.\ transformers or attention mechanisms) to better capture long-range entanglement.
\end{itemize}

Finally, our experiments so far are classical: RDMs are generated from exact diagonalization, tensor networks, or quantum chemistry solvers.
Integrating RD-IP with real NISQ hardware will require careful calibration of noise models and validation against hardware-specific error channels.

\section{Outlook: From Lines to Graphs and Chemistry}

The epure-based, incremental projection perspective developed here is not restricted to one-dimensional chains.
In subsequent work, we extend RulerNet to a GraphRulerNet, replacing the linear GRU carriage by a message-passing network that operates on molecular graphs.
Edges carry local $4 \times 4$ RDM blocks and rich Hamiltonian features; nodes carry orbital or atomic descriptors.
Shina's step control becomes edge-wise and context-aware, incorporating local degree, edge type, and energy sensitivity.

Applied to Fe--S clusters in modest active spaces (e.g.\ Fe$_2$S$_2$ CAS(10,10)), this graph-mode RD-IP achieves high local fidelity ($\sim 0.96$ on dm$_2$ blocks) with sub-minute train+infer cycles on consumer GPUs, but exhibits an energy error floor of $\sim 0.7$~Ha.
We interpret this as a ``variational gap'': the architecture captures the shape of the state but not yet the fine energetic interference.
Bridging this gap will require explicit N-representability layers, richer features, and larger training banks.

Nevertheless, the one-dimensional results presented in this paper establish RD-IP + Shina as a physically grounded, adaptive engine for quantum state reconstruction.
They provide a solid foundation for these graph-based and chemical extensions.

\section*{Acknowledgments}

The author thanks the developers of open-source quantum simulation libraries and numerical linear algebra tools used in this work.
Code prototyping and experimentation were assisted by large language models and related tools, including an OpenAI GPT-5.1 Pro assistant (``ChatGPT Pro 5.1'') for interactive design and debugging, a Codex 5.1-style code generator for boilerplate implementation, and a Gemini 3-based assistant for drafting early versions of explanatory text.
Hyperparameter policies for graph-based extensions were explored with a custom evolutionary framework referred to as \emph{ShinkaEnvolve}.
Any scientific claims, architecture decisions, and interpretations are solely the responsibility of the human author.

\bibliographystyle{unsrt}
% \bibliography{rdip_shina_refs}

\end{document}
